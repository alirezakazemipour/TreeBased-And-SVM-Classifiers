{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">\n",
    "    In the name of God\n",
    "</h3>\n",
    "<p style=\"text-align:left\">\n",
    "    Machine Learning Course 1400, Dr. Sajedi\n",
    "    <br>HW1\n",
    "    <br>Alireza Kazemipour\n",
    "    <br>610300171\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "Here we import python packages that are going to be used over the course of the following code:\n",
    "- *SVC* is the package for performing Support Vector Machine classifier.\n",
    "- *DecisionTreeClassifier* is the package for performing Decision Tree classifier.\n",
    "- *RandomForestClassifier* is the package for performing Random Forest classifier.\n",
    "- *XGBClassifier* is the package for performing XGBoost classifier.\n",
    "- *confusion_matrix* and *classification_report* willbe used to report training and testing metrics.\n",
    "- *pandas* is the package for working with structured `.csv` files.\n",
    "- *matplotlib* is the package for drawing plots.\n",
    "- *seaborn* is the package for drawing attractive and informative statistical graphics.\n",
    "- *tqdm* is the package for displaying progress bars.\n",
    "- *numpy* is the package provided for numerical, algerbric, and etc operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "The dataset was provided with a text file such that it represented a matrix whose rows were specified by the lines of the file and its rows were separated by `,`s. Thus, we read the matrix line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data.txt\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        str_values = line.split(',')\n",
    "        float_values = []\n",
    "        for v in str_values:\n",
    "            float_values.append(float(v))\n",
    "        data.append(float_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input features and labels\n",
    "The first column of the imported dataset was the subjects' numbers so it was removed as it didn't contain any useful information.  \n",
    "Last column of the dataset was representing the labels so, they were gathered in our label vector also, their categorical numbers from `[1, 2, ..., number of classes]` changed to `[0, 1, ..., number of classes - 1]` as it was neccesarry for optimization of imported packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack(data)\n",
    "X = data[:, 1:-1]\n",
    "Y = data[:, -1].astype(int) - 1\n",
    "n_class = np.max(Y) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing parameters\n",
    "Here we fix number of *folds* fo the *k-fold Cross Validation* method. Also, we fix the seed of randomness in our work to produce comparable and reproducible results among different runs or algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_num = 5\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the dataset\n",
    "Here we shuffle the dataset to be able to perform *k-fold Cross Validation* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "shuffler = np.random.permutation(len(X))\n",
    "X = X[shuffler]\n",
    "Y = Y[shuffler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "Here 10% of the total data that we have is randomly reserved for testing learned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.random.permutation(int(0.1 * len(X)))\n",
    "x_test = X[test_idx]\n",
    "y_test = Y[test_idx]\n",
    "\n",
    "X = np.delete(X, test_idx, axis=0)\n",
    "Y = np.delete(Y, test_idx, axis=0)\n",
    "full_batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input standardization\n",
    "We observed that if we standardize the input (which means making it a *zero mean* with *unit variance* distribution) for various algorithms **especially the SVM** then the training phase is done smoother and higher accuracy is more likely to be reached easily. Why? Because, classifying a *zero-mean*-*unit-variance* data (for example by a line) is less sensitive to changes in paramteres.  \n",
    "The following picture shows the concept so that the normalized data is less sensitive to the changes in the slope of the line:\n",
    "<figure>\n",
    "<img src=\"Input_Normalization.jpg\" width=480\\>\n",
    "<!-- <figcaption style=\"text-align:center\">\n",
    "<a href=\"url\">source</a>\n",
    "</figcaption> -->\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # essential for SVM\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "To perform *5-fold cross validation* a python generator has been used such that at each iteration, it picks $\\frac{1}{5}$ of data for validation and the remaining for the training. It performs the eplained procedure by sweeping the data devoted for training phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_fold(x, y, n):\n",
    "    for i in range(n):\n",
    "        x_val = x[i * full_batch_size // cv_num: (i + 1) * full_batch_size // cv_num]\n",
    "        y_val = y[i * full_batch_size // cv_num: (i + 1) * full_batch_size // cv_num]\n",
    "\n",
    "        x_train = np.delete(x, range(i * full_batch_size // cv_num, (i + 1) * full_batch_size // cv_num), axis=0)\n",
    "        y_train = np.delete(y, range(i * full_batch_size // cv_num, (i + 1) * full_batch_size // cv_num), axis=0)\n",
    "\n",
    "        yield x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models' Definition\n",
    "To realize integrity and demonstrating how each model is trained based on an identical bottomline, we have described all of our classification methods here to use them in the training loop later.  \n",
    "The logic behind the this part of the code is that each classifier has a base classifier `base_clf` that all the fixed hyperparameters have been set to it and they won't be changed. Secondly, there a `param` attribute which is a hyperparameter that we're seeking its optimal value. Third, there is `param_values` that specifes the range that the search of optimal hyperparameter should be done with it.\n",
    "\n",
    "#### Now let's take a look at each specific classifier's configuration \n",
    "- Linear SVM\n",
    "1. The *One vs One* strategy is used for the optimization since we're dealing with a multi-class classification and *One vs One* is the choice of preference for this kind of problems.\n",
    "2. The hyperparameter that we're searching for its optimal value is `C`that is proportional to the inverse of regularizer coefficient. Thus, as it increases the regularization effect decreases.\n",
    "3. Only two values of {1, 10} was explored since higher values (in logspace) for `C` make the linear SVM get stuck in an infinite otimization loop.\n",
    "- RBF SVM\n",
    "1. The radial-basis function was used as the non-linear kernel function to perform non-linear classification.\n",
    "2. The *One vs One* strategy is used for the optimization.\n",
    "3. The hyperparameter that we're searching for its optimal value is `C`.\n",
    "4. Search range: `[1, 10, 100, 1000, 1e+4, 1e+5]`.\n",
    "- Decision Tree\n",
    "1. *entropy* criterion was selected for choosing best features during tree's construction\n",
    "2. Since we would like to avoid *overfitting* the hyperparameter that is being sought is `max_depth`. Because if we could find relatively small values for this hyperparameter the, the tree would be shallow, simple and more capable of generalization due to the fact that a lot of branches have been pruned.\n",
    "3. Search range for level of the tree's depth: `[1, 2, ..., 20]\n",
    "- Random Forest\n",
    "1. *entropy* criterion was selected for choosing best features during the forest's tree construction\n",
    "2. `max_depth` of each tree is taken from the `max_depth` that we obtained from *Decision Tree*. (implemented later in the code.)\n",
    "3. Since we would like to avoid *overfitting* the hyperparameter that is being sought is `n_estimators` aka `n_tree`. Because if we could find relatively small values for this hyperparameter the, the forest would be sparse, simple and more capable of generalization due to the fact that a lot of tree have been chopped.\n",
    "4. No bootstraping is allowed and the whole dataset is used for each tree as it was observed that bootstrapping hampers the performance.\n",
    "5. Search range for the number of trees: `[1, 2, ..., 45] \n",
    "- XGBoost\n",
    "1. `max_depth` of each tree is taken from the `max_depth` that we obtained from *Decision Tree*. (implemented later in the code.)\n",
    "2. `n_estimators` of the forest is taken from the `n_estimators` that we obtained from *Random Forest*. (implemented later in the code.)\n",
    "3. The `booster` type was chosen `gbtree` which means individual learners are tree-based\n",
    "4. The `tree_method` was chosen to be `hist` as it is similar to the `exact` method that greedily enumerates all candidate splits however, `hist` is faster.\n",
    "5. Regularizer coefficient chosen to be `1e-3`.\n",
    "6. loss type was chosen `logloss` aka *Categorical Cross Entropy*. (The suitable loos type for multi-class classification.)\n",
    "7. `learning_rate` was the hyperparameter that we performed search on since, it is the most influencial hyperparameter for *XGBoost*.\n",
    "8. Search range for the learning rate: `[1, 5e-1, 1e-2, 5e-3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\"Linear SVM\": {\"base_clf\": SVC(decision_function_shape=\"ovo\",\n",
    "                                       random_state=seed),\n",
    "                       \"param\": \"C\",\n",
    "                       \"param_values\": [1, 10]\n",
    "                       },\n",
    "        \"RBF-SVM\": {\"base_clf\": SVC(kernel=\"rbf\",\n",
    "                                    decision_function_shape=\"ovo\",\n",
    "                                    random_state=seed),\n",
    "                    \"param\": \"C\",\n",
    "                    \"param_values\": [1, 10, 100, 1000, 1e+4, 1e+5]\n",
    "                    },\n",
    "        \"Decision Tree\": {\"base_clf\": DecisionTreeClassifier(criterion=\"entropy\", random_state=seed),\n",
    "                          \"param\": \"max_depth\",\n",
    "                          \"param_values\": np.arange(1, 21)\n",
    "                          },\n",
    "        \"Random Forest\": {\"base_clf\": RandomForestClassifier(criterion=\"entropy\",\n",
    "                                                             random_state=seed,\n",
    "                                                             bootstrap=False),\n",
    "                          \"param\": \"n_estimators\",\n",
    "                          \"param_values\": np.arange(1, 46)\n",
    "                          },\n",
    "        \"XGBoost\": {\"base_clf\": XGBClassifier(booster=\"gbtree\",\n",
    "                                              tree_method=\"hist\",\n",
    "                                              reg_lambda=1e-3,\n",
    "                                              random_state=seed,\n",
    "                                              objective='logloss',\n",
    "                                              use_label_encoder=True,\n",
    "                                              verbosity=0),\n",
    "                    \"param\": \"learning_rate\",\n",
    "                    \"param_values\": [1, 5e-1, 1e-2, 5e-3]\n",
    "                    }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop\n",
    "#### Logic:\n",
    "For every classifier we search over its range of values for the hyperparameter it needs to find. We perform *5-fold cross validation* and the best hyperparameter is the one that its corrrespondig average model during *5-fold cross validation* achieves the highest *validation accuracy*. Then, we report the important training and validation metrics.  \n",
    "Then, performance metrics on the test data that we put aside in the first place is computed and reported. At last, `max_depth` and `n_estimators` of *Decision Tree* and *Random Forest* algorithms are used for further related algorithms such as *XGBoost*.\n",
    "\n",
    "__It's important to note that the Standardization was fitted on the training data and during test phase we used those values as well.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "best_test_acc = -np.inf\n",
    "best_model = None\n",
    "\n",
    "for clf_name, config in clfs.items():\n",
    "    history = {\"train_acc\": [], \"val_acc\": []}\n",
    "    best_val_acc = 0\n",
    "    best_param = None\n",
    "\n",
    "    for p in tqdm(config[\"param_values\"]):\n",
    "        avg_val_acc = 0\n",
    "        avg_train_acc = 0\n",
    "\n",
    "        for x_train, y_train, x_val, y_val in choose_fold(X, Y, cv_num):\n",
    "            clf = config[\"base_clf\"]\n",
    "            setattr(clf, config[\"param\"], p)\n",
    "            clf.fit(x_train, y_train)\n",
    "            y_pred = clf.predict(x_train)\n",
    "            avg_train_acc += (np.sum(y_pred == y_train) / len(y_pred)) * 100\n",
    "            y_pred = clf.predict(x_val)\n",
    "            avg_val_acc += (np.sum(y_pred == y_val) / len(y_pred)) * 100\n",
    "\n",
    "        history[\"train_acc\"].append(avg_train_acc / cv_num)\n",
    "        history[\"val_acc\"].append(avg_val_acc / cv_num)\n",
    "\n",
    "        if history[\"val_acc\"][-1] > best_val_acc:\n",
    "            best_val_acc = history[\"val_acc\"][-1]\n",
    "            best_param = p\n",
    "\n",
    "    print(f\"==> {clf_name} <==\")\n",
    "    print(\"Training result:\")\n",
    "    print(\"\\tbest param: {} = {}\\n\\tbest validation accuracy = {:.2f}%\".format(config[\"param\"],\n",
    "                                                                               best_param,\n",
    "                                                                               best_val_acc))\n",
    "    plt.plot(config[\"param_values\"], history[\"train_acc\"], c=\"r\")\n",
    "    plt.plot(config[\"param_values\"], history[\"val_acc\"], c=\"b\")\n",
    "    plt.legend(history.keys())\n",
    "    plt.grid()\n",
    "    plt.title(clf_name)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(config[\"param\"])\n",
    "    if \"SVM\" in clf_name or clf_name == \"XGBoost\":\n",
    "        plt.xscale(\"log\")\n",
    "    plt.show()\n",
    "    print(\"Test result: \")\n",
    "    setattr(clf, config[\"param\"], best_param)\n",
    "    clf.fit(X, Y)\n",
    "    x_test_stand = scaler.transform(x_test)\n",
    "    y_pred = clf.predict(x_test_stand)\n",
    "    test_acc = (np.sum(y_pred == y_test) / len(y_pred)) * 100\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_model = clf_name\n",
    "    con_mat = confusion_matrix(y_test, y_pred)\n",
    "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    con_mat_df = pd.DataFrame(con_mat_norm, index=[i for i in range(n_class)], columns=[i for i in range(n_class)])\n",
    "    figure = plt.figure(figsize=(n_class, n_class))\n",
    "    sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('confusion matrix')\n",
    "    plt.show()\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    if clf_name == \"Decision Tree\":\n",
    "        setattr(clfs[\"Random Forest\"][\"base_clf\"], \"max_depth\", best_param)\n",
    "        setattr(clfs[\"XGBoost\"][\"base_clf\"], \"max_depth\", best_param)\n",
    "\n",
    "    if clf_name == \"Random Forest\":\n",
    "        setattr(clfs[\"XGBoost\"][\"base_clf\"], \"n_estimators\", best_param)\n",
    "        \n",
    "print(best_model)\n",
    "print(best_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model\n",
    "Since the dataset is quite __balanced__, the *accuracy* metric on the test set can be good measure for choosing the best model therefore, the best model is {} wth accuracy = {}\n",
    "\n",
    "### metrics and confusion matrix\n",
    "\n",
    "## Tabel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "answer_and_report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
